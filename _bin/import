#!/usr/bin/env ruby

require 'nokogiri'
require 'rexml/document'
require 'date'
require 'fileutils'
require 'net/http'
require 'uri'
require 'cgi'

$globalauthor = "Max Rydahl Andersen"
$blogurl = "http://blog.xam.dk"

class String
  # The extended characters map used by removeaccents. The accented characters 
  # are coded here using their numerical equivalent to sidestep encoding issues.
  # These correspond to ISO-8859-1 encoding.
  ACCENTS_MAPPING = {
    'E' => [200,201,202,203],
    'e' => [232,233,234,235],
    'A' => [192,193,194,195,196,197],
    'a' => [224,225,226,227,228,229,230],
    'C' => [199],
    'c' => [231],
    'O' => [210,211,212,213,214,216],
    'o' => [242,243,244,245,246,248],
    'I' => [204,205,206,207],
    'i' => [236,237,238,239],
    'U' => [217,218,219,220],
    'u' => [249,250,251,252],
    'N' => [209],
    'n' => [241],
    'Y' => [221],
    'y' => [253,255],
    'AE' => [306],
    'ae' => [346],
    'OE' => [188],
    'oe' => [189]
  }
  
  
  # Remove the accents from the string. Uses String::ACCENTS_MAPPING as the source map.
  def removeaccents    
    str = String.new(self)
    String::ACCENTS_MAPPING.each {|letter,accents|
      packed = accents.pack('U*')
      rxp = Regexp.new("[#{packed}]", nil)
      str.gsub!(rxp, letter)
    }
    
    str
  end
  
  
  # Convert a string to a format suitable for a URL without ever using escaped characters.
  # It calls strip, removeaccents, downcase (optional) then removes the spaces (optional)
  # and finally removes any characters matching the default regexp (/[^-_A-Za-z0-9]/).
  #
  # Options
  #
  # * :downcase => call downcase on the string (defaults to true)
  # * :convert_spaces => Convert space to underscore (defaults to false)
  # * :regexp => The regexp matching characters that will be converting to an empty string (defaults to /[^-_A-Za-z0-9]/)
  def urlize(options = {})
    options[:downcase] ||= true
    options[:convert_spaces] ||= false
    options[:regexp] ||= /[^-_A-Za-z0-9]/
    
    str = self.strip.removeaccents
    str.downcase! if options[:downcase]
    str.gsub!(/\ /,'-') if options[:convert_spaces]
    str.gsub(options[:regexp], '')
  end

  # This follows the generated ID rules
  def anchorize(options = {})
    options[:downcase] ||= true
    options[:convert_spaces] ||= false
    options[:regexp] ||= /[^-_A-Za-z0-9]/
    
    str = self.strip.removeaccents
    str.downcase! if options[:downcase]
    str.gsub!(/\ /,'_') if options[:convert_spaces]
    str.gsub(options[:regexp], '')
  end
end


class Importer

  def initialize(import_file, output_dir)
    @import_file = import_file
    @output_dir  = output_dir
  end

  def import_file
    doc = REXML::Document.new( File.read( @import_file ) )
    root = doc.root
    print "Start importing"
    root.get_elements( '/feed/entry' ).each do |item|
      print "Import\n"
      import_item( item )
    end
  end

  private

  def import_item(item_xml)
    #type = item_xml.get_text( 'wp:post_type' )
    #return if type == 'page'
    #status = item_xml.get_text( 'wp:status' )
    #return if status != 'publish'

    #print item_xml
    tags = Array.new
    item_xml.get_elements( 'category' ).each do |tag|
      
      value = tag.attributes['term']
      print "tag: " + value
      if value != "Uncategorized"
        tags << value
      end
    end

    
    title    = item_xml.get_text( 'title' ).to_s
    author   = $globalauthor
    link     = item_xml.get_text( 'link' ).to_s

    
    if ( link =~ %r(/([^/]+)/$) )
      slug = $1
    else
      slug = title.to_s.urlize({:convert_spaces=>true})
    end

    print "Title: " + title

    published = DateTime.parse( item_xml.get_text( 'published' ).to_s )
    #content  = REXML::Text.unnormalize( item_xml.get_text( 'content:encoded' ).to_s )
    content  = item_xml.get_text( 'summary' ).to_s
    #print "First Content: #{content}"
    ## jive seem to double encode entities...trying to fix them.
    content.gsub!("&lt;","<")
    content.gsub!("&gt;",">")
    content.gsub!("&amp;","&")
    in_pre = false
    p_content = ''
    content.each_line do |line|
      #if ( line =~ /<pre>/ )
      #  in_pre = true
      #end
      #if ( line =~ /<\/pre>/ )
      #  in_pre = false
      #end
      #if ( in_pre || ( line =~ /^\s*</ ) )
      #  p_content << line
      #elsif ( line.strip == '' )
      #  # nothing
      #else
      #  #p_content << "<p>#{line}</p>\n" 
      #  p_content << "#{line}\n" 
      #end
      if(line.strip == '')
          p_content << "<br/>\n"
          p_content << "<br/>\n"
      else
          p_content << line
      end
    end
    content = p_content

    content = import_images( content )
    content = import_assets( content )

    output_path = File.join( @output_dir, published.strftime( "%Y-%m-%d-#{slug}.html.erb" ) )
    #if ( ! File.exist?( output_path ) )
      puts "writing post: #{output_path}"
      FileUtils.mkdir_p( File.dirname( output_path ) )
      File.open( output_path, 'w' ) do |f|
        #f.puts "date: #{published.year}-#{published.month}-#{published.day}"
        f.puts '---'
        #title.gsub!('"','\\"')
        title.gsub!('\'','\'\'')
        f.puts "title: '#{title}'"
        f.puts "author: '" + $globalauthor + "'"
        f.puts "layout: blog-post"
        f.puts "tags: [ #{tags.join(", ")} ]"
        f.puts "orignallink: '#{link}'"
        f.puts '---'
        f.puts content
      end
    #end

    #puts "RewriteCond %{QUERY_STRING} ^" + link.match(/p=(.*)/)[0].to_s
    #puts "RewriteRule .* http://xam.dk/blog/" + slug

  end

  def import_images(content)
    #doc = REXML::Document.new( '<entry>' + CGI.escapeHTML( content ) + '</entry>' )
    #print "Parsing XXX#{content}XXX"
    doc = Nokogiri::HTML( content )
    #doc = REXML::Document.new( '<entry>' + content  + '</entry>' )
    

    doc.xpath('//img' ).each do |img|
      src = img.attributes['src']

      if ( src =~ /^\// )
        src = $blogurl + "#{src}"
      else
        next
      end

      basename = File.basename( src )
      output_path = "posts/assets/#{basename}"
 
      url = URI.parse( src )

      res = Net::HTTP.start(url.host, url.port) {|http|
        http.get(url.path)
      }

      FileUtils.mkdir_p( File.dirname( output_path ) )
      puts "writing image: #{output_path}"
      File.open( output_path, 'wb' ) do |f|
        f.write res.body
      end

      img.attributes['src'] = "/#{output_path}"
    end
    content = ''
    doc.root.children.each do |c|
      content += c.to_s
    end
    content
  end


  def import_assets(content)
    doc = Nokogiri::HTML( content )
    doc.xpath('//a').each do |a|
      href = a.attributes['href']
      local = false
      if ( href =~ /^\// )
        href = $blogurl + "#{href}"
        local = true
      end
      if ( local && href =~ /\.(pdf)$/ )
        basename = File.basename( href )
        output_path = "posts/assets/#{basename}"
  
        url = URI.parse( href )
  
        if ( ! File.exist?( output_path ) )
          puts "fetching: #{url}"
          FileUtils.mkdir_p( File.dirname( output_path ) )
          File.open( output_path, 'wb' ) do |f|
            Net::HTTP.start(url.host, url.port) do |http|
              http.get(url.path) do |str|
                f.write str
                $stderr.putc '.'
                $stderr.flush
              end
            end
          end
          puts "writing asset: #{output_path}"
        end
  
        a.attributes['href'] = "/#{output_path}"
      end

    end
    content = ''
    doc.root.children.each do |c|
      content += c.to_s
    end
    content

  end


end

if ( $0 == __FILE__ ) 
  importer = Importer.new( ARGV[0], ARGV[1] )
  importer.import_file
end

